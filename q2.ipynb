{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow-datasets"
      ],
      "metadata": {
        "id": "bH-5b915QK8e"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision tree + import library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets import mnist\n",
        "max_depth = 5 # You can change this value as you like\n",
        "# def entropy(y: pd.Series):\n",
        "def entropy(y: pd.Series):\n",
        "  # Calculate the entropy of a series of labels\n",
        "  counts = y.value_counts(normalize=True)\n",
        "  return -np.sum(counts * np.log2(counts))\n",
        "\n",
        "# def information_gain(x: pd.Series, y: pd.Series): return info_gain\n",
        "def information_gain(x: pd.Series, y: pd.Series):\n",
        "  # Calculate the information gain of a feature x given the labels y\n",
        "  info_gain = entropy(y)\n",
        "  for value in x.unique():\n",
        "    y_subset = y[x == value]\n",
        "    info_gain -= len(y_subset) / len(y) * entropy(y_subset)\n",
        "  return info_gain\n",
        "\n",
        "# def information_gains(X: pd.DataFrame, y: pd.Series): return the information gain of all features\n",
        "def information_gains(X: pd.DataFrame, y: pd.Series):\n",
        "  # Calculate the information gain of all features in X given the labels y\n",
        "  info_gains = {}\n",
        "  for feature in X.columns:\n",
        "    info_gains[feature] = information_gain(X[feature], y)\n",
        "  return info_gains\n",
        "\n",
        "# class Node:\n",
        "class Node:\n",
        "  def __init__(self, depth):\n",
        "    # Each node in the tree is an instance of class `Node` which is capable of predicting and fitting.\n",
        "    self.depth = depth\n",
        "    self.best_feature = ''\n",
        "    self.children = []\n",
        "    self.threshold = None\n",
        "    self.choice = None\n",
        "\n",
        "  def _is_leaf(self):\n",
        "    # Check if the node is a leaf node\n",
        "    return len(self.children) == 0\n",
        "\n",
        "  def fit(self, X_train, y_train):\n",
        "    # Fit the node to the training data and split into children if necessary\n",
        "    # If the node is pure or reaches the maximum depth, make it a leaf node\n",
        "    if len(y_train.unique()) == 1 or self.depth == max_depth:\n",
        "      self.choice = y_train.mode()[0]\n",
        "      return\n",
        "\n",
        "    # Find the best feature to split on based on information gain\n",
        "    info_gains = information_gains(X_train, y_train)\n",
        "    self.best_feature = max(info_gains, key=info_gains.get)\n",
        "\n",
        "    # Find the best threshold to split on based on entropy\n",
        "    x_best = X_train[self.best_feature]\n",
        "    thresholds = np.linspace(x_best.min(), x_best.max(), num=10)\n",
        "    entropies = []\n",
        "    for t in thresholds:\n",
        "      y_left = y_train[x_best <= t]\n",
        "      y_right = y_train[x_best > t]\n",
        "      entropy_left = entropy(y_left) if len(y_left) > 0 else 0\n",
        "      entropy_right = entropy(y_right) if len(y_right) > 0 else 0\n",
        "      weighted_entropy = (len(y_left) * entropy_left + len(y_right) * entropy_right) / len(y_train)\n",
        "      entropies.append(weighted_entropy)\n",
        "    self.threshold = thresholds[np.argmin(entropies)]\n",
        "\n",
        "    # Split the data into left and right subsets based on the best feature and threshold\n",
        "    X_left = X_train[x_best <= self.threshold]\n",
        "    y_left = y_train[x_best <= self.threshold]\n",
        "    X_right = X_train[x_best > self.threshold]\n",
        "    y_right = y_train[x_best > self.threshold]\n",
        "\n",
        "    # Create two child nodes and fit them recursively\n",
        "    left_child = Node(self.depth + 1)\n",
        "    right_child = Node(self.depth + 1)\n",
        "    left_child.fit(X_left, y_left)\n",
        "    right_child.fit(X_right, y_right)\n",
        "    self.children = [left_child, right_child]\n",
        "\n",
        "  def predict(self, X):\n",
        "    # Predict the label for a given input\n",
        "    # If the node is a leaf node, return its choice\n",
        "    if self._is_leaf():\n",
        "      return self.choice\n",
        "\n",
        "    # Otherwise, traverse to the left or right child based on the best feature and threshold\n",
        "    x_best = X[self.best_feature]\n",
        "    if x_best <= self.threshold:\n",
        "      return self.children[0].predict(X)\n",
        "    else:\n",
        "      return self.children[1].predict(X)\n",
        "\n",
        "# load mnist.keras.dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# scale data to [0 ,1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# reshape data from 28*28 matrix to 784 array\n",
        "x_train_flat = x_train.reshape(-1, 784)\n",
        "x_test_flat = x_test.reshape(-1, 784)\n",
        "\n",
        "# initializing the pca\n",
        "pca = PCA(n_components=10)\n",
        "\n",
        "# implement pca on our data with 10 component\n",
        "pca.fit(x_train_flat)\n",
        "\n",
        "# select 10 components for train and test data\n",
        "x_train_pca = pca.transform(x_train_flat)\n",
        "x_test_pca = pca.transform(x_test_flat)\n",
        "\n",
        "# convert reduced datasets types to dataframe using pd\n",
        "x_train_pca = pd.DataFrame(x_train_pca)\n",
        "x_test_pca = pd.DataFrame(x_test_pca)\n",
        "y_train = pd.Series(y_train)\n",
        "y_test = pd.Series(y_test)\n",
        "\n",
        "# dt = Node(depth=0)\n",
        "dt = Node(depth=0)\n",
        "\n",
        "# train dt on mnist\n",
        "dt.fit(x_train_pca, y_train)\n",
        "\n",
        "# report model accuracy\n",
        "y_pred = x_test_pca.apply(dt.predict, axis=1)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of decision tree on MNIST with PCA: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w2e9AybQxtx",
        "outputId": "598ea1e6-6513-4b22-dc55-685a7bbb24a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of decision tree on MNIST with PCA: 0.3098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "روند آموزش یک ساعت و سه دقیقه طول کشید و به خاط زمان طولانی آموزش، از کم بودن دقت صرف نظر می شود"
      ],
      "metadata": {
        "id": "gDnorcfFgbua"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}